apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingestion-pipeline-monitor
  labels:
    app: ingestion-pipeline-monitor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ingestion-pipeline-monitor
  template:
    metadata:
      labels:
        app: ingestion-pipeline-monitor
    spec:
      securityContext:
        runAsUser: 1000840000
      containers:
      - name: monitor
        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest
        command:
        - /bin/bash
        - -c
        - |
          # Create Python script
          cat > /tmp/monitor.py << 'EOF'
          import os
          import subprocess
          import json
          import time
          from datetime import datetime

          def log(message):
              print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}", flush=True)

          def monitor_job(kb_name):
              try:
                  # Wait for job to start
                  log("Waiting for job to start...")
                  for _ in range(30):  # Wait up to 30 seconds
                      result = subprocess.run(f"oc get job ingestion-pipeline-{kb_name} -o jsonpath='{{.status.active}}'", shell=True, capture_output=True, text=True)
                      if result.stdout.strip() == "1":
                          log("Job started successfully")
                          break
                      time.sleep(1)
                  else:
                      log("Job did not start within 30 seconds")
                      # Update status to failed
                      update_status(kb_name, 'failed', "Job did not start within 30 seconds")
                      return

                  # Wait for job completion
                  log("Waiting for job completion...")
                  while True:
                      result = subprocess.run(f"oc get job ingestion-pipeline-{kb_name} -o jsonpath='{{.status.conditions[0].type}}'", shell=True, capture_output=True, text=True)
                      if result.stdout.strip() == "Complete":
                          log("Job completed successfully")
                          # Update status to completed
                          update_status(kb_name, 'completed')
                          break
                      elif result.stdout.strip() == "Failed":
                          log("Job failed")
                          # Get job logs for error message
                          logs = subprocess.run(f"oc logs job/ingestion-pipeline-{kb_name}", shell=True, capture_output=True, text=True)
                          error_msg = logs.stdout if logs.stdout else "Job failed without logs"
                          # Update status to failed
                          update_status(kb_name, 'failed', error_msg)
                          break
                      time.sleep(10)
              except Exception as e:
                  log(f"Error monitoring job: {str(e)}")
                  update_status(kb_name, 'failed', str(e))

          def create_ingestion_job(kb_name, s3_config):
              try:
                  log(f"Creating ingestion job for knowledge base: {kb_name}")

                  # Parse S3 config
                  s3_config_dict = json.loads(s3_config)

                  # Create secret for S3 config
                  secret_yaml = f"""
                  apiVersion: v1
                  kind: Secret
                  metadata:
                    name: s3-config-{kb_name}
                    namespace: rag-llama-demo
                  type: Opaque
                  stringData:
                    SOURCE: "S3"
                    EMBEDDING_MODEL: "all-MiniLM-L6-v2"
                    NAME: "{kb_name}"
                    VERSION: "1"
                    ACCESS_KEY_ID: "{s3_config_dict.get('access_key_id', '')}"
                    SECRET_ACCESS_KEY: "{s3_config_dict.get('secret_access_key', '')}"
                    ENDPOINT_URL: "http://minio:9000"
                    BUCKET_NAME: "{s3_config_dict.get('bucket_name', '')}"
                    REGION: "{s3_config_dict.get('region', 'us-east-1')}"
                  """

                  # Add debug logging
                  log(f"S3 Config Dictionary: {s3_config_dict}")
                  log(f"Access Key ID: {s3_config_dict.get('access_key_id', '')}")
                  log(f"Secret Access Key: {s3_config_dict.get('secret_access_key', '')}")
                  log(f"Bucket Name: {s3_config_dict.get('bucket_name', '')}")

                  # Write secret YAML to file
                  with open('/tmp/secret.yaml', 'w') as f:
                      f.write(secret_yaml)

                  # Create secret
                  result = subprocess.run('oc apply -f /tmp/secret.yaml', shell=True, capture_output=True, text=True)
                  if result.returncode != 0:
                      log(f"Error creating secret: {result.stderr}")
                      # Update status to failed
                      update_status(kb_name, 'failed', f"Failed to create secret: {result.stderr}")
                      return
                  log("Secret created successfully")

                  # Create job YAML
                  job_yaml = f"""
                  kind: Job
                  apiVersion: batch/v1
                  metadata:
                    name: ingestion-pipeline-{kb_name}
                    namespace: rag-llama-demo
                    labels:
                      pipelines.kubeflow.org/v2_component: 'true'
                  spec:
                    manualSelector: false
                    backoffLimit: 6
                    completions: 1
                    template:
                      metadata:
                        labels:
                          job-name: ingestion-pipeline-{kb_name}
                          pipelines.kubeflow.org/v2_component: 'true'
                      spec:
                        volumes:
                          - name: ingestion-python-script-volume
                            configMap:
                              name: ingestion-pipeline-script
                              defaultMode: 420
                          - name: dot-local
                            emptyDir: {{}}
                        initContainers:
                          - name: wait-for-pipeline
                            image: 'image-registry.openshift-image-registry.svc:5000/openshift/tools:latest'
                            command:
                              - /bin/bash
                              - '-c'
                              - |
                                set -e
                                url="https://ds-pipeline-dspa:8888/apis/v2beta1/healthz"
                                echo "Waiting for $url..."
                                until curl -ksf "$url"; do
                                  echo "Still waiting for $url ..."
                                  sleep 10
                                done
                                echo "\\nData science pipeline configured."
                            resources: {{}}
                            terminationMessagePath: /dev/termination-log
                            terminationMessagePolicy: File
                            imagePullPolicy: Always
                        containers:
                          - resources: {{}}
                            terminationMessagePath: /dev/termination-log
                            name: create-ingestion-pipeline
                            command:
                              - /bin/bash
                            env:
                              - name: LLAMASTACK_BASE_URL
                                value: 'http://llamastack.rag-llama-demo.svc.cluster.local:8321'
                              - name: DS_PIPELINE_URL
                                value: 'https://ds-pipeline-dspa.rag-llama-demo.svc.cluster.local:8888'
                            envFrom:
                              - secretRef:
                                  name: s3-config-{kb_name}
                            imagePullPolicy: IfNotPresent
                            volumeMounts:
                              - name: ingestion-python-script-volume
                                mountPath: /ingestion-script
                              - name: dot-local
                                mountPath: /.local
                            terminationMessagePolicy: File
                            image: 'python:3.10-slim'
                            args:
                              - '-ec'
                              - |
                                pip install kfp[kubernetes]
                                python3 /ingestion-script/ingestion.py
                        restartPolicy: Never
                        terminationGracePeriodSeconds: 30
                        dnsPolicy: ClusterFirst
                        securityContext: {{}}
                        schedulerName: default-scheduler
                    suspend: false
                    parallelism: 1
                    podReplacementPolicy: TerminatingOrFailed
                    completionMode: NonIndexed
                  """

                  # Write job YAML to file
                  with open('/tmp/job.yaml', 'w') as f:
                      f.write(job_yaml)

                  # Create job
                  result = subprocess.run('oc apply -f /tmp/job.yaml', shell=True, capture_output=True, text=True)
                  if result.returncode != 0:
                      log(f"Error creating job: {result.stderr}")
                      # Update status to failed
                      update_status(kb_name, 'failed', f"Failed to create job: {result.stderr}")
                      return
                  log("Job created successfully")

                  # Start job monitoring in a separate thread
                  import threading
                  monitor_thread = threading.Thread(target=monitor_job, args=(kb_name,))
                  monitor_thread.daemon = True
                  monitor_thread.start()

              except Exception as e:
                  log(f"Error in create_ingestion_job: {str(e)}")
                  # Update status to failed
                  update_status(kb_name, 'failed', str(e))

          def process_database_results():
              try:
                  # Keep track of processed knowledge bases
                  processed_kbs = set()

                  while True:
                      # Read results from shared volume
                      try:
                          with open('/shared/db_results.txt', 'r') as f:
                              results = f.read().strip()
                      except FileNotFoundError:
                          log("No results file found, waiting...")
                          time.sleep(10)
                          continue

                      if not results:
                          log("No new knowledge bases found")
                          time.sleep(10)
                          continue

                      # Process each line
                      for line in results.splitlines():
                          if line.strip():
                              log(f"Processing line: {line}")
                              kb_name, s3_config = line.split('|')
                              kb_name = kb_name.strip()

                              # Skip if already processed
                              if kb_name in processed_kbs:
                                  log(f"Skipping already processed knowledge base: {kb_name}")
                                  continue

                              # Create ingestion job
                              create_ingestion_job(kb_name, s3_config.strip())

                              # Add to processed set
                              processed_kbs.add(kb_name)

                              # Clear the results file after processing
                              with open('/shared/db_results.txt', 'w') as f:
                                  f.write('')

                              log(f"Processed knowledge base: {kb_name}")

                      # Wait before checking again
                      time.sleep(10)

              except Exception as e:
                  log(f"Error processing database results: {str(e)}")
                  time.sleep(10)  # Wait before retrying

          if __name__ == "__main__":
              while True:
                  try:
                      process_database_results()
                  except Exception as e:
                      log(f"Fatal error: {str(e)}")
                      time.sleep(60)  # Wait before retrying
          EOF

          # Make script executable and run it
          chmod +x /tmp/monitor.py
          python3 /tmp/monitor.py
        volumeMounts:
        - name: shared-volume
          mountPath: /shared
      - name: db-query
        image: python:3.10-slim
        command:
        - /bin/bash
        - -c
        - |
          # Create a writable directory for pip installations
          mkdir -p /tmp/pip
          export PIP_TARGET=/tmp/pip
          export PYTHONPATH=/tmp/pip:$PYTHONPATH

          # Install psycopg2
          pip install --no-cache-dir psycopg2-binary

          # Create Python script
          cat > /tmp/query.py << 'EOF'
          import os
          import psycopg2
          import time
          from datetime import datetime

          def log(message):
              print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}", flush=True)

          def check_database():
              try:
                  log("Checking database for new knowledge bases...")

                  # Connect to database
                  conn = psycopg2.connect(
                      host=os.environ["DB_HOST"],
                      database="ai_virtual_assistant",  # Connect directly to ai_virtual_assistant
                      user=os.environ["DB_USER"],
                      password=os.environ["DB_PASSWORD"]
                  )

                  # Query to execute - only get pending knowledge bases
                  query = """
                      SELECT name, source_configuration::text
                      FROM knowledge_bases
                      WHERE is_external = true
                      AND source_configuration IS NOT NULL
                      AND status = 'pending';
                  """
                  log(f"Executing query: {query}")

                  with conn.cursor() as cur:
                      cur.execute(query)
                      results = cur.fetchall()

                  log(f"Found {len(results)} pending knowledge bases")

                  # Write results to shared volume
                  with open('/shared/db_results.txt', 'w') as f:
                      for row in results:
                          f.write(f"{row[0]}|{row[1]}\n")

                  # Update status to 'processing' for the found knowledge bases
                  if results:
                      update_query = """
                          UPDATE knowledge_bases
                          SET status = 'processing'
                          WHERE name = ANY(%s);
                      """
                      kb_names = [row[0] for row in results]
                      with conn.cursor() as cur:
                          cur.execute(update_query, (kb_names,))
                          conn.commit()
                      log(f"Updated status to 'processing' for {len(kb_names)} knowledge bases")

                  conn.close()

              except Exception as e:
                  log(f"Error in check_database: {str(e)}")

          if __name__ == "__main__":
              while True:
                  try:
                      check_database()
                      # Wait for 1 minute before checking again
                      time.sleep(60)
                  except Exception as e:
                      log(f"Fatal error: {str(e)}")
                      time.sleep(60)  # Wait before retrying
          EOF

          # Make script executable and run it
          chmod +x /tmp/query.py
          python3 /tmp/query.py
        env:
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: pgvector
              key: host
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: pgvector
              key: user
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: pgvector
              key: password
        - name: DB_NAME
          valueFrom:
            secretKeyRef:
              name: pgvector
              key: dbname
        volumeMounts:
        - name: shared-volume
          mountPath: /shared
      volumes:
      - name: shared-volume
        emptyDir: {}
      serviceAccountName: ingestion-pipeline-monitor
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingestion-pipeline-monitor
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ingestion-pipeline-monitor
rules:
- apiGroups: [""]
  resources: ["configmaps", "jobs", "secrets"]
  verbs: ["create", "get", "list", "watch", "patch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["create", "get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ingestion-pipeline-monitor
subjects:
- kind: ServiceAccount
  name: ingestion-pipeline-monitor
roleRef:
  kind: Role
  name: ingestion-pipeline-monitor
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ingestion-pipeline-script
data:
  ingestion.py: |
    from kfp import dsl
    from kfp import Client
    from kfp import compiler
    import os

    @dsl.component(
        base_image="python:3.10",
        packages_to_install=[
            "boto3"
        ])
    def fetch_from_s3(output_dir: dsl.OutputPath()):
        import shutil
        import os
        import boto3
        import tempfile
        import json

        # S3 Config
        bucket_name = os.environ.get('BUCKET_NAME')
        minio_endpoint = os.environ.get('ENDPOINT_URL')
        minio_access_key = os.environ.get('ACCESS_KEY_ID')
        minio_secret_key = os.environ.get('SECRET_ACCESS_KEY')

        # Step 1: Download files from MinIO
        temp_dir = tempfile.mkdtemp()
        download_dir = os.path.join(temp_dir, "source_repo")
        os.makedirs(download_dir, exist_ok=True)

        # Connect to MinIO
        print(f"Connecting to MinIO at {minio_endpoint}")
        s3 = boto3.client(
            "s3",
            endpoint_url=minio_endpoint,
            aws_access_key_id=minio_access_key,
            aws_secret_access_key=minio_secret_key,
            verify=False,
            config=boto3.session.Config(
                s3={'addressing_style': 'path'},
                signature_version='s3v4'
            )
        )

        # List and download objects
        paginator = s3.get_paginator("list_objects_v2")
        pages = paginator.paginate(Bucket=bucket_name)

        print(f"Downloading files from bucket: {bucket_name}")
        downloaded_files = []
        for page in pages:
            for obj in page.get("Contents", []):
                key = obj["Key"]
                file_path = os.path.join(download_dir, os.path.basename(key))
                print(f"Downloading: {key} -> {file_path}")
                s3.download_file(bucket_name, key, file_path)
                downloaded_files.append(file_path)

        print(f"Downloaded {len(downloaded_files)} files to {download_dir}")

        if not downloaded_files:
            raise Exception(f"No files found in bucket: {bucket_name}. Please check your bucket configuration.")

        # Ensure the output directory exists
        os.makedirs(output_dir, exist_ok=True)
        print(f"Created output directory: {output_dir}")

        # Print directory contents to debug
        print(f"Contents of temp dir before copy: {os.listdir(temp_dir)}")
        print(f"Contents of download dir before copy: {os.listdir(download_dir)}")

        # Copy all downloaded files to the output directory and create a manifest
        file_manifest = []
        for file_path in downloaded_files:
            if os.path.exists(file_path):
                filename = os.path.basename(file_path)
                output_path = os.path.join(output_dir, filename)

                # Debug output
                print(f"Copying from {file_path} (exists: {os.path.exists(file_path)}) to {output_path}")

                # Copy the file
                shutil.copy2(file_path, output_path)
                file_manifest.append(filename)
                print(f"Successfully copied {file_path} to {output_path}")

        # Write a manifest file to track what was downloaded
        manifest_path = os.path.join(output_dir, "manifest.json")
        with open(manifest_path, "w") as f:
            json.dump(file_manifest, f)

        print(f"Created file manifest at {manifest_path} with {len(file_manifest)} files")
        print(f"Final contents of output dir: {os.listdir(output_dir)}")

        # Clean up the temp directory
        shutil.rmtree(temp_dir, ignore_errors=True)
        print(f"Cleaned up temporary directory: {temp_dir}")

    @dsl.component(
        base_image="python:3.10",
        packages_to_install=[
            "llama-stack-client==0.2.3",
            "docling",
            "docling-core"
        ])
    def process_and_store_pgvector(llamastack_base_url: str, input_dir: dsl.InputPath()):
        import os
        import json

        print(f"Input directory: {input_dir}")
        if os.path.exists(input_dir):
            print(f"Contents of input directory: {os.listdir(input_dir)}")
        else:
            print(f"Input directory does not exist: {input_dir}")

        # Set EasyOCR path to a writeable directory BEFORE importing docling
        os.environ["EASYOCR_MODULE_PATH"] = "/tmp/.EasyOCR"

        from llama_stack_client import LlamaStackClient
        from llama_stack_client.types import Document as LlamaStackDocument

        # Import docling libraries
        from docling.document_converter import DocumentConverter, PdfFormatOption
        from docling.datamodel.base_models import InputFormat
        from docling.datamodel.pipeline_options import PdfPipelineOptions
        from docling_core.transforms.chunker.hybrid_chunker import HybridChunker
        from docling_core.types.doc.labels import DocItemLabel

        # Configuring the vector database
        name = os.environ.get('NAME')
        version = os.environ.get('VERSION')
        embedding_model = os.environ.get('EMBEDDING_MODEL')

        vector_db_name = f"{name}-v{version}".replace(" ", "-").replace(".", "-")

        # Check if vector database already exists
        client = LlamaStackClient(base_url=llamastack_base_url)
        try:
            existing_dbs = client.vector_dbs.list()
            if any(db.vector_db_id == vector_db_name for db in existing_dbs):
                print(f"Vector database {vector_db_name} already exists. Skipping processing.")
                return
        except Exception as e:
            print(f"Error checking existing vector databases: {e}")
            # Continue with processing if we can't check existing databases

        # Read the manifest created by the fetch phase to get the list of files
        manifest_path = os.path.join(input_dir, "manifest.json")
        if not os.path.exists(manifest_path):
            raise FileNotFoundError(f"Manifest file not found at {manifest_path}")

        with open(manifest_path, "r") as f:
            file_manifest = json.load(f)

        print(f"Processing {len(file_manifest)} files from the manifest")

        # Get the full paths of all input files
        input_files = [os.path.join(input_dir, filename) for filename in file_manifest]

        # Verify files exist
        for file_path in input_files:
            if not os.path.exists(file_path):
                print(f"Warning: File doesn't exist: {file_path}")
            else:
                print(f"File exists: {file_path}, size: {os.path.getsize(file_path)} bytes")

        # Step 2: Process files with docling
        # Setup docling components
        pipeline_options = PdfPipelineOptions()
        pipeline_options.generate_picture_images = True
        converter = DocumentConverter(
            allowed_formats=[
                InputFormat.PDF,
                InputFormat.MD,
                InputFormat.DOCX,
                InputFormat.ASCIIDOC,
                InputFormat.JSON_DOCLING,
                InputFormat.HTML
            ],
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        chunker = HybridChunker()
        llama_documents = []
        i = 0
        # Process each file with docling (chunking)
        for file_path in input_files:
            # Skip empty files
            if not os.path.exists(file_path):
                print(f"File doesn't exist: {file_path}")
                continue

            file_size = os.path.getsize(file_path)
            if file_size == 0:
                print(f"Skipping empty file: {file_path} (0 bytes)")
                continue

            print(f"Processing {file_path} with docling...")
            try:
                docling_doc = converter.convert(source=file_path).document
                chunks = chunker.chunk(docling_doc)
                chunk_count = 0

                for chunk in chunks:
                    if any(
                        c.label in [DocItemLabel.TEXT, DocItemLabel.PARAGRAPH, DocItemLabel.TABLE,
                                  DocItemLabel.PAGE_HEADER, DocItemLabel.PAGE_FOOTER,
                                  DocItemLabel.TITLE, DocItemLabel.PICTURE, DocItemLabel.CHART,
                                  DocItemLabel.DOCUMENT_INDEX, DocItemLabel.SECTION_HEADER]
                        for c in chunk.meta.doc_items
                    ):
                        i += 1
                        chunk_count += 1
                        llama_documents.append(
                            LlamaStackDocument(
                                document_id=f"doc-{i}",
                                content=chunk.text,
                                mime_type='text/plain',
                                metadata={"source": os.path.basename(file_path)},
                            )
                        )
                print(f"Created {chunk_count} chunks from {file_path}")

            except Exception as e:
                error_message = str(e)
                print(f"Error processing {file_path}: {error_message}")

        total_chunks = len(llama_documents)
        print(f"Total valid chunks prepared: {total_chunks}")

        # Add error handling for zero chunks
        if total_chunks == 0:
            raise Exception("No valid chunks were created. Check document processing errors above.")

        # Step 3: Register vector database and store chunks with embeddings
        client = LlamaStackClient(base_url=llamastack_base_url)
        print("Registering db")
        try:
            client.vector_dbs.register(
                vector_db_id=vector_db_name,
                embedding_model=embedding_model,
                embedding_dimension=384,
                provider_id="pgvector",
            )
            print("Vector DB registered successfully")
        except Exception as e:
            error_message = str(e)
            print(f"Failed to register vector DB: {error_message}")
            raise Exception(f"Vector DB registration failed: {error_message}")

        try:
            print(f"Inserting {total_chunks} chunks into vector database")
            client.tool_runtime.rag_tool.insert(
                documents=llama_documents,
                vector_db_id=vector_db_name,
                chunk_size_in_tokens=512,
            )
            print("Documents successfully inserted into the vector DB")

        except Exception as e:
            print("Embedding insert failed:", e)
            raise Exception(f"Failed to insert documents into vector DB: {e}")

    @dsl.component(
        base_image="python:3.10",
        packages_to_install=[
            "llama-stack-client==0.2.3",
            "docling",
            "docling-core"
        ])
    def urls_to_pgvector(llamastack_base_url: str):
        import os
        import json

        # Set EasyOCR path to a writeable directory BEFORE importing docling
        os.environ["EASYOCR_MODULE_PATH"] = "/tmp/.EasyOCR"

        from llama_stack_client import LlamaStackClient
        from llama_stack_client.types import Document as LlamaStackDocument

        # Import docling libraries
        from docling.document_converter import DocumentConverter, PdfFormatOption
        from docling.datamodel.base_models import InputFormat
        from docling.datamodel.pipeline_options import PdfPipelineOptions
        from docling_core.transforms.chunker.hybrid_chunker import HybridChunker
        from docling_core.types.doc.labels import DocItemLabel

        # Configuring the vector database
        name = os.environ.get('NAME')
        version = os.environ.get('VERSION')
        embedding_model = os.environ.get('EMBEDDING_MODEL')

        vector_db_name = f"{name}-v{version}".replace(" ", "-").replace(".", "-")

        # Check if vector database already exists
        client = LlamaStackClient(base_url=llamastack_base_url)
        try:
            existing_dbs = client.vector_dbs.list()
            if any(db.vector_db_id == vector_db_name for db in existing_dbs):
                print(f"Vector database {vector_db_name} already exists. Skipping processing.")
                return
        except Exception as e:
            print(f"Error checking existing vector databases: {e}")
            # Continue with processing if we can't check existing databases

        # Step 2: Process files with docling
        # Setup docling components
        pipeline_options = PdfPipelineOptions()
        pipeline_options.generate_picture_images = True
        converter = DocumentConverter(
            allowed_formats=[
                InputFormat.PDF,
                InputFormat.MD,
                InputFormat.DOCX,
                InputFormat.ASCIIDOC,
                InputFormat.JSON_DOCLING,
                InputFormat.HTML
            ],
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        chunker = HybridChunker()
        llama_documents = []
        input_files = os.environ.get('URLS', '')
        input_files = input_files.strip('[]').split()
        print(f"Input files: {input_files}")
        i = 0
        # Process each file with docling (chunking)
        for file_path in input_files:
            print(f"Processing {file_path} with docling...")
            try:
                docling_doc = converter.convert(source=file_path).document
                chunks = chunker.chunk(docling_doc)
                chunk_count = 0

                for chunk in chunks:
                    if any(
                        c.label in [DocItemLabel.TEXT, DocItemLabel.PARAGRAPH, DocItemLabel.TABLE,
                                  DocItemLabel.PAGE_HEADER, DocItemLabel.PAGE_FOOTER,
                                  DocItemLabel.TITLE, DocItemLabel.PICTURE, DocItemLabel.CHART,
                                  DocItemLabel.DOCUMENT_INDEX, DocItemLabel.SECTION_HEADER]
                        for c in chunk.meta.doc_items
                    ):
                        i += 1
                        chunk_count += 1
                        llama_documents.append(
                            LlamaStackDocument(
                                document_id=f"doc-{i}",
                                content=chunk.text,
                                mime_type='text/plain',
                                metadata={"source": os.path.basename(file_path)},
                            )
                        )
                print(f"Created {chunk_count} chunks from {file_path}")

            except Exception as e:
                error_message = str(e)
                print(f"Error processing {file_path}: {error_message}")

        total_chunks = len(llama_documents)
        print(f"Total valid chunks prepared: {total_chunks}")

        # Add error handling for zero chunks
        if total_chunks == 0:
            raise Exception("No valid chunks were created. Check document processing errors above.")

        # Step 3: Register vector database and store chunks with embeddings
        client = LlamaStackClient(base_url=llamastack_base_url)
        print("Registering db")
        try:
            client.vector_dbs.register(
                vector_db_id=vector_db_name,
                embedding_model=embedding_model,
                embedding_dimension=384,
                provider_id="pgvector",
            )
            print("Vector DB registered successfully")
        except Exception as e:
            error_message = str(e)
            print(f"Failed to register vector DB: {error_message}")
            raise Exception(f"Vector DB registration failed: {error_message}")

        try:
            print(f"Inserting {total_chunks} chunks into vector database")
            client.tool_runtime.rag_tool.insert(
                documents=llama_documents,
                vector_db_id=vector_db_name,
                chunk_size_in_tokens=512,
            )
            print("Documents successfully inserted into the vector DB")

        except Exception as e:
            print("Embedding insert failed:", e)
            raise Exception(f"Failed to insert documents into vector DB: {e}")

    @dsl.pipeline(name="fetch-and-store-pipeline")
    def s3_pipeline():
      from kfp import kubernetes
      secret_key_to_env = {
              'SOURCE': 'SOURCE',
              'EMBEDDING_MODEL': 'EMBEDDING_MODEL',
              'NAME': 'NAME',
              'VERSION': 'VERSION',
              'ACCESS_KEY_ID': 'ACCESS_KEY_ID',
              'SECRET_ACCESS_KEY': 'SECRET_ACCESS_KEY',
              'ENDPOINT_URL': 'ENDPOINT_URL',
              'BUCKET_NAME': 'BUCKET_NAME',
              'REGION': 'REGION'
      }

      fetch_task = fetch_from_s3()
      fetch_task.set_caching_options(False)

      process_task = process_and_store_pgvector(
          llamastack_base_url=os.environ["LLAMASTACK_BASE_URL"],
          input_dir=fetch_task.outputs["output_dir"]
      )
      process_task.set_caching_options(False)

      kubernetes.use_secret_as_env(
          task=fetch_task,
          secret_name=f"s3-config-{os.environ['NAME']}",
          secret_key_to_env=secret_key_to_env)

      kubernetes.use_secret_as_env(
          task=process_task,
          secret_name=f"s3-config-{os.environ['NAME']}",
          secret_key_to_env=secret_key_to_env)

    @dsl.pipeline(name="fetch-and-store-pipeline")
    def url_pipeline():
        from kfp import kubernetes
        secret_key_to_env = {
            'SOURCE': 'SOURCE',
            'EMBEDDING_MODEL': 'EMBEDDING_MODEL',
            'NAME': 'NAME',
            'VERSION': 'VERSION',
            'URLS': 'URLS'
        }

        process_task = urls_to_pgvector(
            llamastack_base_url=os.environ["LLAMASTACK_BASE_URL"]
        )
        process_task.set_caching_options(False)

        kubernetes.use_secret_as_env(
            task=process_task,
            secret_name=f"s3-config-{os.environ['NAME']}",
            secret_key_to_env=secret_key_to_env
        )

    # 1. Compile pipeline to a file
    pipeline_yaml = "/tmp/fetch_chunk_embed_pipeline.yaml"

    if os.environ.get('SOURCE') == "S3":
        print("S3 pipeline")
        compiler.Compiler().compile(
            pipeline_func=s3_pipeline,
            package_path=pipeline_yaml
        )
    elif os.environ.get('SOURCE') == "URL":
        print("URL pipeline")
        compiler.Compiler().compile(
            pipeline_func=url_pipeline,
            package_path=pipeline_yaml
        )

    import os
    # 2. Connect to KFP
    client = Client(
      host=os.environ["DS_PIPELINE_URL"],
      verify_ssl=False
    )

    # 3. Upload pipeline
    uploaded_pipeline = client.upload_pipeline(
      pipeline_package_path=pipeline_yaml,
      pipeline_name=f"ingestion-pipeline-{os.environ['NAME']}"
    )

    # 4. Run the pipeline
    run = client.create_run_from_pipeline_package(
      pipeline_file=pipeline_yaml,
      arguments={},
      run_name=f"ingestion-run-{os.environ['NAME']}"
    )

    print(f"Pipeline submitted! Run ID: {run.run_id}")
